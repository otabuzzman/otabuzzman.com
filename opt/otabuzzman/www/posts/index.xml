<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>  on otabuzzman&#39;s blog</title>
    <link>http://localhost:1313/posts/</link>
    <description>Recent content in   on otabuzzman&#39;s blog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Jun 2025 08:23:48 +0200</lastBuildDate>
    <atom:link href="http://localhost:1313/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Articles</title>
      <link>http://localhost:1313/posts/my-articles/</link>
      <pubDate>Thu, 12 Jun 2025 08:23:48 +0200</pubDate>
      <guid>http://localhost:1313/posts/my-articles/</guid>
      <description>&lt;p class=&#34;pb2&#34;&gt;From time to time I write articles for computer magazines. Topic ideas come to me by chance, for example by reading news feeds or social media. For me, writing an article with the aim of making a topic understandable to others is a great way to really understand that topic. Click on the cover images below to get to the texts.&lt;/p&gt;&#xA;&#xA;&lt;div class=&#34;w-100 h5 mb6 h4-m h3-l mb4-m mb4-l&#34;&gt;&#xA;  &lt;div class=&#34;fl w-100 w-third-ns pa2&#34;&gt;&lt;a href=&#34;https://www.heise.de/select/mac-and-i/2025/3/2505513453451868721&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;macandi-cover-2025-03.webp&#34;/&gt;&lt;/a&gt;&lt;/div&gt;&#xA;  &lt;div class=&#34;fl w-100 w-third-ns pa2&#34;&gt;&lt;a href=&#34;https://www.heise.de/select/ix/2025/2/2428312405840089505&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;ix-cover-2025-02.webp&#34;/&gt;&lt;/a&gt;&lt;/div&gt;&#xA;  &lt;div class=&#34;fl w-100 w-third-ns pa2&#34;&gt;&lt;a href=&#34;https://www.heise.de/select/mac-and-i/2024/6/2422608104680975514&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;macandi-cover-2024-06.webp&#34;/&gt;&lt;/a&gt;&lt;/div&gt;&#xA;  &lt;div class=&#34;fl w-100 w-third-ns pa2&#34;&gt;&lt;a href=&#34;https://www.heise.de/select/ix/2024/9/2415813332268253375&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;ix-cover-2024-09.webp&#34;/&gt;&lt;/a&gt;&lt;/div&gt;&#xA;  &lt;div class=&#34;fl w-100 w-third-ns pa2&#34;&gt;&lt;a href=&#34;https://www.heise.de/select/mac-and-i/2024/4/2405115483018713222&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;macandi-cover-2024-04.webp&#34;/&gt;&lt;/a&gt;&lt;/div&gt;&#xA;  &lt;div class=&#34;fl w-100 w-third-ns pa2&#34;&gt;&lt;a href=&#34;https://www.heise.de/select/ix/2024/8/2413416363596097808&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;ix-cover-2024-08.webp&#34;/&gt;&lt;/a&gt;&lt;/div&gt;&#xA;  &lt;div class=&#34;fl w-100 w-third-ns pa2&#34;&gt;&lt;a href=&#34;https://www.heise.de/select/mac-and-i/2024/1/2326112085400864712&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;macandi-cover-2024-01.webp&#34;/&gt;&lt;/a&gt;&lt;/div&gt;&#xA;  &lt;div class=&#34;fl w-100 w-third-ns pa2&#34;&gt;&lt;a href=&#34;https://www.heise.de/select/ix/2024/3/2332713580068863270&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;ix-cover-2024-03.webp&#34;/&gt;&lt;/a&gt;&lt;/div&gt;&#xA;  &lt;div class=&#34;fl w-100 w-third-ns pa2&#34;&gt;&lt;a href=&#34;https://www.heise.de/select/ix/2024/2/2332508044372163045&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;ix-cover-2024-02.webp&#34;/&gt;&lt;/a&gt;&lt;/div&gt;&#xA;  &lt;div class=&#34;fl w-100 w-third-ns pa2&#34;&gt;&lt;a href=&#34;https://www.heise.de/select/mac-and-i/2023/2/2230010240877679302&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;macandi-cover-2023-02.webp&#34;/&gt;&lt;/a&gt;&lt;/div&gt;&#xA;  &lt;div class=&#34;fl w-100 w-third-ns pa2&#34;&gt;&lt;a href=&#34;https://www.heise.de/select/ix/2023/9/2317208544348813229&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;ix-cover-2023-09.webp&#34;/&gt;&lt;/a&gt;&lt;/div&gt;&#xA;  &lt;div class=&#34;fl w-100 w-third-ns pa2&#34;&gt;&lt;a href=&#34;https://www.heise.de/select/ix/2023/5/2306110183371943158&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;ix-cover-2023-05.webp&#34;/&gt;&lt;/a&gt;&lt;/div&gt;&#xA;  &lt;div class=&#34;fl w-100 w-third-ns pa2&#34;&gt;&lt;a href=&#34;https://www.heise.de/select/ix/2023/2/2228708105605963640&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;ix-cover-2023-02.webp&#34;/&gt;&lt;/a&gt;&lt;/div&gt;&#xA;  &lt;div class=&#34;fl w-100 w-third-ns pa2&#34;&gt;&lt;a href=&#34;https://www.heise.de/select/ix/2023/1/2226509241088177927&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;ix-cover-2023-01.webp&#34;/&gt;&lt;/a&gt;&lt;/div&gt;&#xA;  &lt;div class=&#34;fl w-100 w-third-ns pa2&#34;&gt;&lt;a href=&#34;https://www.heise.de/select/mac-and-i/2022/5/2209113283329077435&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;macandi-cover-2022-05.webp&#34;/&gt;&lt;/a&gt;&lt;/div&gt;&#xA;  &lt;div class=&#34;fl w-100 w-third-ns pa2&#34;&gt;&lt;a href=&#34;https://www.heise.de/select/ix/2022/7/2210109535103124472&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;ix-cover-2022-07.webp&#34;/&gt;&lt;/a&gt;&lt;/div&gt;&#xA;  &lt;div class=&#34;fl w-100 w-third-ns pa2&#34;&gt;&lt;a href=&#34;https://www.heise.de/select/ix/2022/5/2204509371592906361&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;ix-cover-2022-05.webp&#34;/&gt;&lt;/a&gt;&lt;/div&gt;&#xA;  &lt;div class=&#34;fl w-100 w-third-ns pa2&#34;&gt;&lt;a href=&#34;https://www.heise.de/select/ct/2018/21/1539056664307657&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;ct-cover-2018-23.webp&#34;/&gt;&lt;/a&gt;&lt;/div&gt;&#xA;  &lt;div class=&#34;fl w-100 w-third-ns pa2&#34;&gt;&lt;a href=&#34;https://www.heise.de/select/ix/2017/9/1503952623245752&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;ix-cover-2017-09.webp&#34;/&gt;&lt;/a&gt;&lt;/div&gt;&#xA;  &lt;div class=&#34;fl w-100 w-third-ns pa2&#34;&gt;&lt;a href=&#34;ix.07.01.091-094.pdf&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;ix-cover-2007-01.webp&#34;/&gt;&lt;/a&gt;&lt;/div&gt;&#xA;  &lt;div class=&#34;fl w-100 w-third-ns pa2&#34;&gt;&lt;a href=&#34;ix.06.07.118-120.pdf&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;ix-cover-2006-07.webp&#34;/&gt;&lt;/a&gt;&lt;/div&gt;&#xA;  &lt;div class=&#34;fl w-100 w-third-ns pa2&#34;&gt;&lt;a href=&#34;ix.05.05.098-101.pdf&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;ix-cover-2005-05.webp&#34;/&gt;&lt;/a&gt;&lt;/div&gt;&#xA;  &lt;div class=&#34;fl w-100 w-third-ns pa2&#34;&gt;&lt;a href=&#34;ix.05.01.146-149.pdf&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;ix-cover-2005-01.webp&#34;/&gt;&lt;/a&gt;&lt;/div&gt;&#xA;  &lt;div class=&#34;fl w-100 w-third-ns pa2&#34;&gt;&lt;a href=&#34;ix.04.09.124-127.pdf&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;ix-cover-2004-09.webp&#34;/&gt;&lt;/a&gt;&lt;/div&gt;&#xA;  &lt;div class=&#34;fl w-100 w-third-ns pa2&#34;&gt;&lt;a href=&#34;ix.04.06.064-067.pdf&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;ix-cover-2004-06.webp&#34;/&gt;&lt;/a&gt;&lt;/div&gt;&#xA;  &lt;div class=&#34;fl w-100 w-third-ns pa2&#34;&gt;&lt;a href=&#34;ix9801112/art.htm&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;dummy_cover.webp&#34;/&gt;&lt;/a&gt;&lt;/div&gt;&#xA;&lt;/div&gt;</description>
    </item>
    <item>
      <title>Metal parallelization of llm.c</title>
      <link>http://localhost:1313/posts/metal-llmswift/</link>
      <pubDate>Sun, 12 Jan 2025 19:44:21 +0100</pubDate>
      <guid>http://localhost:1313/posts/metal-llmswift/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://developer.apple.com/metal/&#34;&gt;Metal&lt;/a&gt; is Apple&amp;rsquo;s low-level API for GPU programming and &lt;a href=&#34;https://github.com/karpathy/llm.c&#34;&gt;llm.c&lt;/a&gt; is Andrej Karpathy&amp;rsquo;s plain C and CUDA implementation of GPT-2. The C version leverages &lt;a href=&#34;https://www.openmp.org/&#34;&gt;OpenMP&lt;/a&gt; to parallelize the layer functions on the CPU cores. The CUDA version is highly optimized for multi-node multi-accelerator parallelization on NVIDIA GPUs using &lt;a href=&#34;https://www.open-mpi.org/&#34;&gt;Open MPI&lt;/a&gt; and &lt;a href=&#34;https://developer.nvidia.com/nccl&#34;&gt;NCCL&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;I once ported the C version to Swift and used &lt;a href=&#34;https://developer.apple.com/documentation/DISPATCH&#34;&gt;Grand Central Dispatch&lt;/a&gt; (GCD) for CPU parallelization. The Xcode project is in &lt;a href=&#34;https://github.com/otabuzzman/llm.swift&#34;&gt;llm.swift&lt;/a&gt;. Despite using the &lt;code&gt;-Ounchecked&lt;/code&gt; Swift compiler option to generate fast code without bounds checks the C version compiled with &lt;code&gt;clang&lt;/code&gt; runs about 6 times faster than Swift.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Run llm.c in TornadoVM</title>
      <link>http://localhost:1313/posts/tornado-llmc/</link>
      <pubDate>Mon, 28 Oct 2024 16:58:13 +0200</pubDate>
      <guid>http://localhost:1313/posts/tornado-llmc/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://github.com/beehive-lab/TornadoVM&#34;&gt;TornadoVM&lt;/a&gt; lets Java programs execute on accelerators. &lt;a href=&#34;https://github.com/karpathy/llm.c&#34;&gt;llm.c&lt;/a&gt; is a plain C implementation of OpenAI‘s GPT-2, the LLM that powered the 1st ChatGPT. Released in fall ‘22, it sparked an AI hype that still lasts. Both are not a perfect fit at first glance, but a Java version of llm.c could make them friends, so I tried to bring them together.&lt;/p&gt;&#xA;&lt;p&gt;Although there was already a Java port of llm.c, I made my own to get (back) into the groove of Java. I defined some obvious classes, turned C functions into Java methods, replaced pointers with array inidices, used Java Streams instead of OpenMP to parallelize &lt;code&gt;for&lt;/code&gt;-loops, and leveraged the Java Vector API for matrix multiplication (the latter taken from &lt;a href=&#34;https://github.com/mukel/llama2.java&#34;&gt;llama2.java&lt;/a&gt;, thx for sharing &lt;a href=&#34;https://github.com/mukel&#34;&gt;@TheMukel&lt;/a&gt;).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Parallel Java with TornadoVM (3)</title>
      <link>http://localhost:1313/posts/tornado-java-3/</link>
      <pubDate>Mon, 22 Apr 2024 13:15:17 +0200</pubDate>
      <guid>http://localhost:1313/posts/tornado-java-3/</guid>
      <description>&lt;p&gt;In my original setup, the PJ2 library is pre-installed in a separate folder, so I installed TornadoVM the same way. The installation is well &lt;a href=&#34;https://tornadovm.readthedocs.io/en/latest/installation.html&#34;&gt;documented&lt;/a&gt; and essentially requires cloning the repository and running the installer. &lt;strong&gt;Note&lt;/strong&gt; that the script uses Python and installs some modules. To keep the Python system configuration clean from this stuff I prefer setting up a Python &lt;em&gt;Virtual Environment&lt;/em&gt; before running the installer.&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git clone https://github.com/beehive-lab/TornadoVM.git&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cd TornadoVM&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# create Python Virtual Environment (venv)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# python -m venv .venv&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# activate venv for bash&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# source .venv/bin/activate&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;bin/tornadovm-installer --jdk jdk21 --backend opencl,ptx,spirv&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once finished (after a couple of minutes and tons of logging on screen) there is a batch file &lt;code&gt;setvars.sh&lt;/code&gt; that must be sourced to run the &lt;code&gt;tornado&lt;/code&gt; CLI, and ask it to list the available accelerators.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Parallel Java with TornadoVM (2)</title>
      <link>http://localhost:1313/posts/tornado-java-2/</link>
      <pubDate>Thu, 18 Apr 2024 13:15:17 +0200</pubDate>
      <guid>http://localhost:1313/posts/tornado-java-2/</guid>
      <description>&lt;p&gt;This second part of the report focuses on the changes necessary to separate a section of code and run it on an accelerator. Since I used TornadoVM&amp;rsquo;s &lt;a href=&#34;https://tornadovm.readthedocs.io/en/latest/programming.html#loop-parallel-api&#34;&gt;&lt;em&gt;Loop Parallel API&lt;/em&gt;&lt;/a&gt; the code in question is in the body of a for-loop.&lt;/p&gt;&#xA;&lt;p&gt;In my case the for-loop is inside an instance method of a nested class and shares variables with that class and also with the parent class. I had to change this concept because TornadoVM requires static methods for parallelization. So I defined method parameters for all shared variables and let TornadoVM pass them to the kernel. Some of these variables have multiple elements (e.g. matrices). I put these in memory buffers that TornadoVM passes to the accelerator.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Parallel Java with CUDA</title>
      <link>http://localhost:1313/posts/parallel-java/</link>
      <pubDate>Thu, 11 Apr 2024 23:38:31 +0200</pubDate>
      <guid>http://localhost:1313/posts/parallel-java/</guid>
      <description>&lt;p&gt;An infographic about my first approach to parallelizing Java code in 2017. It worked for me then and probably still does, but now there are tools that are much easier to use and are also much more flexible. One is &lt;a href=&#34;https://www.tornadovm.org/&#34;&gt;TornadoVM&lt;/a&gt;, which essentially allows the programmer to mark up the code to be parallelized and does all the heavy lifting for execution on popular accelerators (AMD, Intel, NVIDIA) and multiple CPU cores. I created a tutorial on TornadoVM that was published in a German computer magazine: part &lt;a href=&#34;https://www.heise.de/select/ix/2024/2/2332508044372163045&#34;&gt;one&lt;/a&gt; and &lt;a href=&#34;https://www.heise.de/select/ix/2024/3/2332713580068863270&#34;&gt;two&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Run X11 Clients on Tablet</title>
      <link>http://localhost:1313/posts/x11-on-tablet/</link>
      <pubDate>Mon, 09 May 2022 19:35:12 +0200</pubDate>
      <guid>http://localhost:1313/posts/x11-on-tablet/</guid>
      <description>&lt;p&gt;A friend of mine recently told me about porting a VNC server to a microcontroller. We joked about it and I capped it off by saying I&amp;rsquo;ve been thinking about porting X11 to iOS. I ended up using VNC instead, but my buddy said there would probably already be an X11 port for iOS - and he was right. Mocha X11 is an X server implementation for tablets. It is available in the App and Play Stores. There are lite versions with a session interval of five minutes. Too short for serious work, but enough to get a taste of what it&amp;rsquo;s like to see decades-old software meet modern devices.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Parallel Java with TornadoVM</title>
      <link>http://localhost:1313/posts/tornado-java-1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/tornado-java-1/</guid>
      <description>&lt;p&gt;A few years ago I wrote a Java app that creates star maps (&lt;a href=&#34;https://chartacaeli.org/artistic-star-chart.pdf&#34;&gt;example&lt;/a&gt;). It does this by projecting the coordinates of celestial bodies onto a flat canvas. One of the features is to map images of artistic representations of certain star constellations onto the maps. The approach I took to perform the required calculations turned out to be quite slow.&lt;/p&gt;&#xA;&lt;p&gt;When I heard about &lt;a href=&#34;https://developer.nvidia.com/cuda-toolkit&#34;&gt;CUDA&lt;/a&gt; I was excited by the idea of doing computations on graphics cards. I wondered if my slow sequential Java code could be run much faster in parallel on a GPU.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
