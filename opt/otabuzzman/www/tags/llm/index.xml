<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LLM on otabuzzman&#39;s blog</title>
    <link>http://localhost:1313/tags/llm/</link>
    <description>Recent content in LLM on otabuzzman&#39;s blog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 12 Jan 2025 19:44:21 +0100</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/llm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Metal parallelization of llm.c</title>
      <link>http://localhost:1313/posts/metal-llmswift/</link>
      <pubDate>Sun, 12 Jan 2025 19:44:21 +0100</pubDate>
      <guid>http://localhost:1313/posts/metal-llmswift/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://developer.apple.com/metal/&#34;&gt;Metal&lt;/a&gt; is Apple&amp;rsquo;s low-level API for GPU programming and &lt;a href=&#34;https://github.com/karpathy/llm.c&#34;&gt;llm.c&lt;/a&gt; is Andrej Karpathy&amp;rsquo;s plain C and CUDA implementation of GPT-2. The C version leverages &lt;a href=&#34;https://www.openmp.org/&#34;&gt;OpenMP&lt;/a&gt; to parallelize the layer functions on the CPU cores. The CUDA version is highly optimized for multi-node multi-accelerator parallelization on NVIDIA GPUs using &lt;a href=&#34;https://www.open-mpi.org/&#34;&gt;Open MPI&lt;/a&gt; and &lt;a href=&#34;https://developer.nvidia.com/nccl&#34;&gt;NCCL&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;I once ported the C version to Swift and used &lt;a href=&#34;https://developer.apple.com/documentation/DISPATCH&#34;&gt;Grand Central Dispatch&lt;/a&gt; (GCD) for CPU parallelization. The Xcode project is in &lt;a href=&#34;https://github.com/otabuzzman/llm.swift&#34;&gt;llm.swift&lt;/a&gt;. Despite using the &lt;code&gt;-Ounchecked&lt;/code&gt; Swift compiler option to generate fast code without bounds checks the C version compiled with &lt;code&gt;clang&lt;/code&gt; runs about 6 times faster than Swift.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Run llm.c in TornadoVM</title>
      <link>http://localhost:1313/posts/tornado-llmc/</link>
      <pubDate>Mon, 28 Oct 2024 16:58:13 +0200</pubDate>
      <guid>http://localhost:1313/posts/tornado-llmc/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://github.com/beehive-lab/TornadoVM&#34;&gt;TornadoVM&lt;/a&gt; lets Java programs execute on accelerators. &lt;a href=&#34;https://github.com/karpathy/llm.c&#34;&gt;llm.c&lt;/a&gt; is a plain C implementation of OpenAI‘s GPT-2, the LLM that powered the 1st ChatGPT. Released in fall ‘22, it sparked an AI hype that still lasts. Both are not a perfect fit at first glance, but a Java version of llm.c could make them friends, so I tried to bring them together.&lt;/p&gt;&#xA;&lt;p&gt;Although there was already a Java port of llm.c, I made my own to get (back) into the groove of Java. I defined some obvious classes, turned C functions into Java methods, replaced pointers with array inidices, used Java Streams instead of OpenMP to parallelize &lt;code&gt;for&lt;/code&gt;-loops, and leveraged the Java Vector API for matrix multiplication (the latter taken from &lt;a href=&#34;https://github.com/mukel/llama2.java&#34;&gt;llama2.java&lt;/a&gt;, thx for sharing &lt;a href=&#34;https://github.com/mukel&#34;&gt;@TheMukel&lt;/a&gt;).&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
